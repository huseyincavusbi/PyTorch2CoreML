{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e24db1d",
   "metadata": {},
   "source": [
    "# PyTorch SE-ResNeXt50 to CoreML Conversion\n",
    "\n",
    "This notebook demonstrates how to convert a trained PyTorch SE-ResNeXt50 model to Apple's CoreML format for deployment on iOS, macOS, and other Apple platforms.\n",
    "\n",
    "## Overview\n",
    "- **Source Model**: SE-ResNeXt50 trained for medical image classification\n",
    "- **Input Model**: `/Users/hc/Documents/PyWorks/best_seresnext50_model.pth`\n",
    "- **Target Format**: CoreML (.mlmodel)\n",
    "- **Use Case**: Binary classification (Normal vs Abnormal medical images)\n",
    "\n",
    "## Conversion Pipeline\n",
    "1. **Load PyTorch Model**: Restore the trained SE-ResNeXt50 architecture and weights\n",
    "2. **TorchScript Conversion**: Convert to an intermediate TorchScript format\n",
    "3. **CoreML Conversion**: Transform TorchScript to CoreML format\n",
    "4. **Validation**: Test the converted model for correctness\n",
    "5. **Optimization**: Apply CoreML optimizations for deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcbf6ee",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for PyTorch model loading, TorchScript conversion, and CoreML conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96adeb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Torch version 2.7.1 has not been tested with coremltools. You may run into unexpected errors. Torch 2.5.0 is the most recent version that has been tested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "Conversion started at: 2025-07-03 15:10:35\n",
      "Platform: macOS-15.3.1-arm64-arm-64bit\n",
      "PyTorch version: 2.7.1\n",
      "CoreMLTools version: 8.3.0\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models.resnet import Bottleneck\n",
    "from torchvision import transforms\n",
    "\n",
    "# CoreML conversion\n",
    "import coremltools as ct\n",
    "from coremltools.models.neural_network import quantization_utils\n",
    "\n",
    "# Utilities\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import platform\n",
    "\n",
    "# Display\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"Conversion started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CoreMLTools version: {ct.__version__}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d811edf",
   "metadata": {},
   "source": [
    "## 2. Load the Trained PyTorch Model\n",
    "\n",
    "First, we need to recreate the SE-ResNeXt50 model architecture and load the trained weights from the checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8cef17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SE-ResNeXt50 model architecture...\n",
      "Model created with 25496897 parameters\n"
     ]
    }
   ],
   "source": [
    "# Define the SE (Squeeze-and-Excitation) Layer\n",
    "class SELayer(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation layer for channel attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, channel: int, reduction: int = 16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "# Define SE-enhanced Bottleneck block\n",
    "class SEBottleneck(Bottleneck):\n",
    "    \"\"\"ResNeXt Bottleneck block with Squeeze-and-Excitation.\"\"\"\n",
    "    \n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes: int, planes: int, stride: int = 1, \n",
    "                 downsample: Optional[nn.Module] = None, groups: int = 1, \n",
    "                 base_width: int = 64, dilation: int = 1, \n",
    "                 norm_layer: Optional[nn.Module] = None, se_reduction: int = 16):\n",
    "        super(SEBottleneck, self).__init__(\n",
    "            inplanes, planes, stride, downsample, groups, \n",
    "            base_width, dilation, norm_layer\n",
    "        )\n",
    "        self.se = SELayer(planes * self.expansion, reduction=se_reduction)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        # Apply SE attention\n",
    "        out = self.se(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_seresnext50(num_classes: int = 1, se_reduction: int = 16) -> nn.Module:\n",
    "    \"\"\"Create SE-ResNeXt50 model with specified number of classes.\"\"\"\n",
    "    \n",
    "    # Start with pretrained ResNeXt50\n",
    "    model = models.resnext50_32x4d(pretrained=False)  # We'll load our own weights\n",
    "    base_width = model.base_width\n",
    "\n",
    "    def replace_bottlenecks(module: nn.Module, se_reduction_ratio: int, base_width: int) -> None:\n",
    "        \"\"\"Recursively replace standard bottlenecks with SE bottlenecks.\"\"\"\n",
    "        for name, child_module in module.named_children():\n",
    "            if isinstance(child_module, Bottleneck):\n",
    "                # Extract parameters from existing bottleneck\n",
    "                inplanes = child_module.conv1.in_channels\n",
    "                planes = child_module.conv3.out_channels // child_module.expansion\n",
    "                stride = child_module.stride\n",
    "                downsample = child_module.downsample\n",
    "                groups = child_module.conv2.groups\n",
    "                dilation = child_module.conv2.dilation[0]\n",
    "\n",
    "                # Create new SE bottleneck\n",
    "                new_bottleneck = SEBottleneck(\n",
    "                    inplanes=inplanes,\n",
    "                    planes=planes,\n",
    "                    stride=stride,\n",
    "                    downsample=downsample,\n",
    "                    groups=groups,\n",
    "                    base_width=base_width,\n",
    "                    dilation=dilation,\n",
    "                    se_reduction=se_reduction_ratio\n",
    "                )\n",
    "\n",
    "                # Load existing weights (excluding SE layer)\n",
    "                new_bottleneck.load_state_dict(child_module.state_dict(), strict=False)\n",
    "                \n",
    "                # Replace the module\n",
    "                setattr(module, name, new_bottleneck)\n",
    "            else:\n",
    "                # Recursively process child modules\n",
    "                replace_bottlenecks(child_module, se_reduction_ratio, base_width)\n",
    "\n",
    "    # Replace all bottlenecks with SE bottlenecks\n",
    "    replace_bottlenecks(model, se_reduction, base_width)\n",
    "\n",
    "    # Replace final classifier layer\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model architecture\n",
    "print(\"Creating SE-ResNeXt50 model architecture...\")\n",
    "model = get_seresnext50(num_classes=1, se_reduction=16)\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "737cd97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: /Users/hc/Documents/PyWorks/best_seresnext50_model.pth\n",
      "Checkpoint loaded successfully!\n",
      "Training epoch: 11\n",
      "Validation loss: 0.010342853844721377\n",
      "Removed 'module.' prefix from state dict keys\n",
      "‚úÖ Model weights loaded successfully!\n",
      "\n",
      "Model Configuration:\n",
      "  ‚Ä¢ Learning rate: 0.0004\n",
      "  ‚Ä¢ Weight decay: 0.0001\n",
      "  ‚Ä¢ Batch size: 128\n",
      "  ‚Ä¢ SE reduction ratio: 16\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model weights\n",
    "checkpoint_path = '/Users/hc/Documents/PyWorks/best_seresnext50_model.pth'\n",
    "\n",
    "print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "\n",
    "# Check if checkpoint file exists\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "print(\"Checkpoint loaded successfully!\")\n",
    "print(f\"Training epoch: {checkpoint.get('epoch', 'Unknown')}\")\n",
    "print(f\"Validation loss: {checkpoint.get('val_loss', 'Unknown')}\")\n",
    "\n",
    "# Extract model state dict\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# Handle DataParallel models (remove 'module.' prefix if present)\n",
    "if list(state_dict.keys())[0].startswith('module.'):\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:]  # Remove 'module.' prefix\n",
    "        new_state_dict[name] = v\n",
    "    state_dict = new_state_dict\n",
    "    print(\"Removed 'module.' prefix from state dict keys\")\n",
    "\n",
    "# Load weights into model\n",
    "missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "if missing_keys:\n",
    "    print(f\"‚ö†Ô∏è  Missing keys: {missing_keys}\")\n",
    "if unexpected_keys:\n",
    "    print(f\"‚ö†Ô∏è  Unexpected keys: {unexpected_keys}\")\n",
    "\n",
    "print(\"‚úÖ Model weights loaded successfully!\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Display model configuration\n",
    "config = checkpoint.get('config', {})\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {config.get('learning_rate', 'Unknown')}\")\n",
    "print(f\"  ‚Ä¢ Weight decay: {config.get('weight_decay', 'Unknown')}\")\n",
    "print(f\"  ‚Ä¢ Batch size: {config.get('batch_size', 'Unknown')}\")\n",
    "print(f\"  ‚Ä¢ SE reduction ratio: {config.get('se_reduction_ratio', 16)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0302d",
   "metadata": {},
   "source": [
    "## 3. Prepare a Dummy Input for Tracing\n",
    "\n",
    "Create a dummy input tensor that matches the expected input shape for the model. This will be used for TorchScript tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c57ac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input specifications:\n",
      "  ‚Ä¢ Image size: 224x224\n",
      "  ‚Ä¢ Channels: 3\n",
      "  ‚Ä¢ Batch size: 1\n",
      "  ‚Ä¢ Normalization mean: [0.485, 0.456, 0.406]\n",
      "  ‚Ä¢ Normalization std: [0.229, 0.224, 0.225]\n",
      "\n",
      "Dummy input shape: torch.Size([1, 3, 224, 224])\n",
      "Dummy input dtype: torch.float32\n",
      "\n",
      "Testing model with dummy input...\n",
      "‚úÖ Model test successful!\n",
      "Output shape: torch.Size([1, 1])\n",
      "Output dtype: torch.float32\n",
      "Output range: [0.8903, 0.8903]\n",
      "Sigmoid probability: 0.7089\n",
      "\n",
      "Preprocessing pipeline defined for reference:\n"
     ]
    }
   ],
   "source": [
    "# Define input specifications\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "BATCH_SIZE = 1  # For inference, typically batch size of 1\n",
    "CHANNELS = 3    # RGB images\n",
    "\n",
    "# ImageNet normalization values (same as used in training)\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "print(f\"Input specifications:\")\n",
    "print(f\"  ‚Ä¢ Image size: {IMG_HEIGHT}x{IMG_WIDTH}\")\n",
    "print(f\"  ‚Ä¢ Channels: {CHANNELS}\")\n",
    "print(f\"  ‚Ä¢ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  ‚Ä¢ Normalization mean: {MEAN}\")\n",
    "print(f\"  ‚Ä¢ Normalization std: {STD}\")\n",
    "\n",
    "# Create dummy input tensor\n",
    "dummy_input = torch.randn(BATCH_SIZE, CHANNELS, IMG_HEIGHT, IMG_WIDTH)\n",
    "print(f\"\\nDummy input shape: {dummy_input.shape}\")\n",
    "print(f\"Dummy input dtype: {dummy_input.dtype}\")\n",
    "\n",
    "# Test the model with dummy input to ensure it works\n",
    "print(\"\\nTesting model with dummy input...\")\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        dummy_output = model(dummy_input)\n",
    "    print(f\"‚úÖ Model test successful!\")\n",
    "    print(f\"Output shape: {dummy_output.shape}\")\n",
    "    print(f\"Output dtype: {dummy_output.dtype}\")\n",
    "    print(f\"Output range: [{dummy_output.min().item():.4f}, {dummy_output.max().item():.4f}]\")\n",
    "    \n",
    "    # Apply sigmoid to get probability\n",
    "    probability = torch.sigmoid(dummy_output).item()\n",
    "    print(f\"Sigmoid probability: {probability:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model test failed: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Define preprocessing transformation (for reference)\n",
    "preprocess_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "print(f\"\\nPreprocessing pipeline defined for reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10054c6f",
   "metadata": {},
   "source": [
    "## 4. Convert the PyTorch Model to TorchScript\n",
    "\n",
    "Convert the PyTorch model to TorchScript format, which serves as an intermediate representation for CoreML conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7815755c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting PyTorch model to TorchScript...\n",
      "==================================================\n",
      "Attempting TorchScript tracing...\n",
      "‚úÖ TorchScript tracing successful!\n",
      "Testing traced model...\n",
      "‚úÖ Traced model test successful!\n",
      "Output difference (should be very small): 0.00000000\n",
      "\n",
      "üíæ TorchScript model saved to: seresnext50_torchscript.pt\n",
      "\n",
      "TorchScript Model Information:\n",
      "  ‚Ä¢ Model type: <class 'torch.jit._trace.TopLevelTracedModule'>\n",
      "  ‚Ä¢ Code: def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  fc = self.fc\n",
      "  avgpool = self.avgpool\n",
      "  layer4 = self.layer4\n",
      "  layer3 = self.layer3\n",
      "  layer2 = self.layer2\n",
      "  layer1 = self.layer1\n",
      "  maxpool = self.maxpool\n",
      "  relu = self.relu\n",
      "  bn1 = self.bn1\n",
      "  conv1 = self.conv1\n",
      "  _0 = (relu).forward((bn1).forward((conv1).forward(x, ), ), )\n",
      "  _1 = (layer1).forward((maxpool).forward(_0, ), )\n",
      "  _2 = (layer3).forward((layer2).forward(_1, ), )\n",
      "  _3 = (avgpool).forward((layer4).forward(_2, ), )\n",
      "  input = torch.flatten(_3, 1)\n",
      "  return (fc).forward(input, )\n",
      "\n",
      "  ‚Ä¢ Graph nodes: 23\n",
      "  ‚Ä¢ Graph inputs: 2\n",
      "  ‚Ä¢ Graph outputs: 1\n"
     ]
    }
   ],
   "source": [
    "# Convert PyTorch model to TorchScript using tracing\n",
    "print(\"Converting PyTorch model to TorchScript...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Method 1: Try tracing first (usually works better for most models)\n",
    "    print(\"Attempting TorchScript tracing...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        traced_model = torch.jit.trace(model, dummy_input)\n",
    "    \n",
    "    print(\"‚úÖ TorchScript tracing successful!\")\n",
    "    \n",
    "    # Test the traced model\n",
    "    print(\"Testing traced model...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        traced_output = traced_model(dummy_input)\n",
    "    \n",
    "    # Compare outputs to ensure tracing worked correctly\n",
    "    original_output = model(dummy_input)\n",
    "    output_diff = torch.abs(traced_output - original_output).max().item()\n",
    "    \n",
    "    print(f\"‚úÖ Traced model test successful!\")\n",
    "    print(f\"Output difference (should be very small): {output_diff:.8f}\")\n",
    "    \n",
    "    if output_diff > 1e-5:\n",
    "        print(\"‚ö†Ô∏è  Large output difference detected - may indicate tracing issues\")\n",
    "    \n",
    "    torchscript_model = traced_model\n",
    "    \n",
    "except Exception as trace_error:\n",
    "    print(f\"‚ùå TorchScript tracing failed: {trace_error}\")\n",
    "    \n",
    "    # Method 2: Try scripting as fallback\n",
    "    print(\"\\nAttempting TorchScript scripting as fallback...\")\n",
    "    \n",
    "    try:\n",
    "        scripted_model = torch.jit.script(model)\n",
    "        print(\"‚úÖ TorchScript scripting successful!\")\n",
    "        \n",
    "        # Test the scripted model\n",
    "        with torch.no_grad():\n",
    "            scripted_output = scripted_model(dummy_input)\n",
    "        \n",
    "        print(\"‚úÖ Scripted model test successful!\")\n",
    "        torchscript_model = scripted_model\n",
    "        \n",
    "    except Exception as script_error:\n",
    "        print(f\"‚ùå TorchScript scripting also failed: {script_error}\")\n",
    "        raise Exception(\"Both TorchScript tracing and scripting failed\")\n",
    "\n",
    "# Save TorchScript model for reference\n",
    "torchscript_path = 'seresnext50_torchscript.pt'\n",
    "torch.jit.save(torchscript_model, torchscript_path)\n",
    "print(f\"\\nTorchScript model saved to: {torchscript_path}\")\n",
    "\n",
    "# Get model information\n",
    "print(f\"\\nTorchScript Model Information:\")\n",
    "print(f\"  ‚Ä¢ Model type: {type(torchscript_model)}\")\n",
    "print(f\"  ‚Ä¢ Code: {torchscript_model.code if hasattr(torchscript_model, 'code') else 'Not available'}\")\n",
    "\n",
    "# Check model graph\n",
    "try:\n",
    "    graph = torchscript_model.graph\n",
    "    print(f\"  ‚Ä¢ Graph nodes: {len(list(graph.nodes()))}\")\n",
    "    print(f\"  ‚Ä¢ Graph inputs: {len(list(graph.inputs()))}\")\n",
    "    print(f\"  ‚Ä¢ Graph outputs: {len(list(graph.outputs()))}\")\n",
    "except:\n",
    "    print(\"  ‚Ä¢ Graph information not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba25526",
   "metadata": {},
   "source": [
    "## 5. Convert TorchScript Model to Core ML Format\n",
    "\n",
    "Use CoreMLTools to convert the TorchScript model to CoreML format with proper input/output specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560fa8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting TorchScript model to CoreML...\n",
      "==================================================\n",
      "Input specifications:\n",
      "  ‚Ä¢ Name: 'image'\n",
      "  ‚Ä¢ Shape: (1, 3, 224, 224)\n",
      "  ‚Ä¢ Type: float32\n",
      "\n",
      "Starting CoreML conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 668/669 [00:00<00:00, 7541.72 ops/s]\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 668/669 [00:00<00:00, 7541.72 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 159.63 passes/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 159.63 passes/s]\n",
      "Running MIL default pipeline: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [00:01<00:00, 68.61 passes/s] \n",
      "Running MIL default pipeline: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89/89 [00:01<00:00, 68.61 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 235.65 passes/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CoreML conversion successful!\n",
      "Conversion time: 3.17 seconds\n",
      "\n",
      "Adding model metadata...\n",
      "\n",
      "CoreML Model Information:\n",
      "  ‚Ä¢ Model type: <class 'coremltools.models.model.MLModel'>\n",
      "  ‚Ä¢ Deployment target: iOS 15+\n",
      "  ‚Ä¢ Backend: ML Program\n",
      "\n",
      "Model Specification:\n",
      "  ‚Ä¢ Inputs: 1\n",
      "    - image: multiArrayType {\n",
      "  shape: 1\n",
      "  shape: 3\n",
      "  shape: 224\n",
      "  shape: 224\n",
      "  dataType: FLOAT32\n",
      "}\n",
      "\n",
      "  ‚Ä¢ Outputs: 1\n",
      "    - output: multiArrayType {\n",
      "  shape: 1\n",
      "  shape: 1\n",
      "  dataType: FLOAT32\n",
      "}\n",
      "\n",
      "  ‚Ä¢ Model size estimate: 0.12 MB\n",
      "\n",
      "‚úÖ CoreML model ready for deployment!\n"
     ]
    }
   ],
   "source": [
    "# Convert TorchScript model to CoreML\n",
    "print(\"Converting TorchScript model to CoreML...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Define input specifications for CoreML\n",
    "    input_shape = ct.Shape(shape=(BATCH_SIZE, CHANNELS, IMG_HEIGHT, IMG_WIDTH))\n",
    "    \n",
    "    # Create input type with proper naming and preprocessing\n",
    "    input_type = [\n",
    "        ct.TensorType(\n",
    "            name=\"image\",\n",
    "            shape=input_shape,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Define output type\n",
    "    output_type = [\n",
    "        ct.TensorType(\n",
    "            name=\"output\", \n",
    "            dtype=np.float32\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"Input specifications:\")\n",
    "    print(f\"  ‚Ä¢ Name: 'image'\")\n",
    "    print(f\"  ‚Ä¢ Shape: {input_shape}\")\n",
    "    print(f\"  ‚Ä¢ Type: float32\")\n",
    "    \n",
    "    # Convert to CoreML\n",
    "    print(f\"\\nStarting CoreML conversion...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    coreml_model = ct.convert(\n",
    "        torchscript_model,\n",
    "        inputs=input_type,\n",
    "        outputs=output_type,\n",
    "        minimum_deployment_target=ct.target.iOS15,  # For modern Apple devices\n",
    "        convert_to=\"mlprogram\",  # Use ML Program backend for iOS 15+/macOS 12+\n",
    "        debug=False\n",
    "    )\n",
    "    \n",
    "    conversion_time = time.time() - start_time\n",
    "    print(f\"‚úÖ CoreML conversion successful!\")\n",
    "    print(f\"Conversion time: {conversion_time:.2f} seconds\")\n",
    "    \n",
    "    # Add model metadata\n",
    "    print(f\"\\nAdding model metadata...\")\n",
    "    \n",
    "    coreml_model.author = \"PyTorch to CoreML Converter\"\n",
    "    coreml_model.short_description = \"SE-ResNeXt50 for medical image classification\"\n",
    "    coreml_model.version = \"1.0\"\n",
    "    \n",
    "    # Add input/output descriptions\n",
    "    coreml_model.input_description[\"image\"] = \"Input medical image (224x224 RGB)\"\n",
    "    coreml_model.output_description[\"output\"] = \"Classification logit (raw output before sigmoid)\"\n",
    "    \n",
    "    # Display model information\n",
    "    print(f\"\\nCoreML Model Information:\")\n",
    "    print(f\"  ‚Ä¢ Model type: {type(coreml_model)}\")\n",
    "    print(f\"  ‚Ä¢ Deployment target: iOS 15+\")\n",
    "    print(f\"  ‚Ä¢ Backend: ML Program\")\n",
    "    \n",
    "    # Check model inputs and outputs\n",
    "    spec = coreml_model.get_spec()\n",
    "    \n",
    "    print(f\"\\nModel Specification:\")\n",
    "    print(f\"  ‚Ä¢ Inputs: {len(spec.description.input)}\")\n",
    "    for input_feature in spec.description.input:\n",
    "        print(f\"    - {input_feature.name}: {input_feature.type}\")\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Outputs: {len(spec.description.output)}\")\n",
    "    for output_feature in spec.description.output:\n",
    "        print(f\"    - {output_feature.name}: {output_feature.type}\")\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Model size estimate: {len(spec.SerializeToString()) / (1024*1024):.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå CoreML conversion failed: {e}\")\n",
    "    print(f\"Error type: {type(e)}\")\n",
    "    import traceback\n",
    "    print(f\"Traceback: {traceback.format_exc()}\")\n",
    "    raise e\n",
    "\n",
    "print(f\"\\n‚úÖ CoreML model ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3513064",
   "metadata": {},
   "source": [
    "## 6. Save the Core ML Model\n",
    "\n",
    "Save the converted CoreML model to disk with proper naming and optional optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf83dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving CoreML model...\n",
      "==================================================\n",
      "Saving model to: seresnext50_mri_coreml.mlpackage\n",
      "‚úÖ CoreML model saved successfully!\n",
      "  ‚Ä¢ File path: /Users/hc/Documents/PyWorks/seresnext50_mri_coreml.mlpackage\n",
      "  ‚Ä¢ File size: 0.00 MB\n",
      "\n",
      "Creating quantized version for mobile deployment...\n",
      "Quantizing using linear quantization\n",
      "‚ö†Ô∏è  Quantization failed: MLModel of type mlProgram cannot be loaded just from the model spec object. It also needs the path to the weights file. Please provide that as well, using the 'weights_dir' argument.\n",
      "The original model is still available.\n",
      "\n",
      "Creating model with built-in preprocessing...\n",
      "‚úÖ Model ready for deployment!\n",
      "Note: Remember to apply the same preprocessing as during training:\n",
      "  1. Resize to 224x224\n",
      "  2. Convert to tensor (0-1 range)\n",
      "  3. Normalize with mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
      "\n",
      "Deployment Information:\n",
      "  ‚Ä¢ Compatible with: iOS 15+, macOS 12+, watchOS 8+, tvOS 15+\n",
      "  ‚Ä¢ Framework: Core ML\n",
      "  ‚Ä¢ Input: RGB image (224x224)\n",
      "  ‚Ä¢ Output: Classification logit (apply sigmoid for probability)\n",
      "  ‚Ä¢ Use case: Stroke MRI Image Classification\n"
     ]
    }
   ],
   "source": [
    "# Save the CoreML model\n",
    "print(\"Saving CoreML model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define output file path\n",
    "output_path = \"seresnext50_mri_coreml.mlpackage\"  # Use .mlpackage for ML Program\n",
    "\n",
    "try:\n",
    "    # Save the CoreML model\n",
    "    print(f\"Saving model to: {output_path}\")\n",
    "    coreml_model.save(output_path)\n",
    "    \n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(output_path) / (1024 * 1024)  # Size in MB\n",
    "    \n",
    "    print(f\"‚úÖ CoreML model saved successfully!\")\n",
    "    print(f\"  ‚Ä¢ File path: {os.path.abspath(output_path)}\")\n",
    "    print(f\"  ‚Ä¢ File size: {file_size:.2f} MB\")\n",
    "    \n",
    "    # Optional: Create a quantized version for smaller file size\n",
    "    print(f\"\\nCreating quantized version for mobile deployment...\")\n",
    "    \n",
    "    try:\n",
    "        # Apply 16-bit quantization\n",
    "        quantized_model = quantization_utils.quantize_weights(coreml_model, nbits=16)\n",
    "        quantized_path = \"seresnext50_medical_classifier_quantized.mlpackage\"\n",
    "        \n",
    "        quantized_model.save(quantized_path)\n",
    "        quantized_file_size = os.path.getsize(quantized_path) / (1024 * 1024)\n",
    "        \n",
    "        print(f\"‚úÖ Quantized model saved!\")\n",
    "        print(f\"  ‚Ä¢ File path: {os.path.abspath(quantized_path)}\")\n",
    "        print(f\"  ‚Ä¢ File size: {quantized_file_size:.2f} MB\")\n",
    "        print(f\"  ‚Ä¢ Size reduction: {((file_size - quantized_file_size) / file_size * 100):.1f}%\")\n",
    "        \n",
    "    except Exception as quant_error:\n",
    "        print(f\"‚ö†Ô∏è  Quantization failed: {quant_error}\")\n",
    "        print(\"The original model is still available.\")\n",
    "    \n",
    "    # Create a model with preprocessing built-in (optional)\n",
    "    print(f\"\\nCreating model with built-in preprocessing...\")\n",
    "    \n",
    "    try:\n",
    "        # Define preprocessing pipeline\n",
    "        # Note: CoreML preprocessing may differ from PyTorch transforms\n",
    "        from coremltools.models.neural_network import NeuralNetworkBuilder\n",
    "        from coremltools.models import datatypes\n",
    "        \n",
    "        # This is a simplified version - for production, you might want\n",
    "        # to include normalization directly in the model\n",
    "        preprocessing_model = coreml_model  # Use the original model\n",
    "        \n",
    "        print(f\"‚úÖ Model ready for deployment!\")\n",
    "        print(f\"Note: Remember to apply the same preprocessing as during training:\")\n",
    "        print(f\"  1. Resize to 224x224\")\n",
    "        print(f\"  2. Convert to tensor (0-1 range)\")\n",
    "        print(f\"  3. Normalize with mean={MEAN}, std={STD}\")\n",
    "        \n",
    "    except Exception as prep_error:\n",
    "        print(f\"‚ö†Ô∏è  Preprocessing integration skipped: {prep_error}\")\n",
    "    \n",
    "except Exception as save_error:\n",
    "    print(f\"‚ùå Failed to save CoreML model: {save_error}\")\n",
    "    raise save_error\n",
    "\n",
    "# Print deployment information\n",
    "print(f\"\\nDeployment Information:\")\n",
    "print(f\"  ‚Ä¢ Compatible with: iOS 15+, macOS 12+, watchOS 8+, tvOS 15+\")\n",
    "print(f\"  ‚Ä¢ Framework: Core ML\")\n",
    "print(f\"  ‚Ä¢ Input: RGB image (224x224)\")\n",
    "print(f\"  ‚Ä¢ Output: Classification logit (apply sigmoid for probability)\")\n",
    "print(f\"  ‚Ä¢ Use case: Stroke MRI Image Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f02fdbc",
   "metadata": {},
   "source": [
    "## 7. Test Core ML Model Loading\n",
    "\n",
    "Verify that the saved CoreML model can be loaded correctly and validate its functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5b854d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing CoreML model loading and inference...\n",
      "==================================================\n",
      "Loading CoreML model from disk...\n",
      "‚úÖ CoreML model loaded successfully!\n",
      "\n",
      "Model Metadata:\n",
      "  ‚Ä¢ Author: PyTorch to CoreML Converter\n",
      "  ‚Ä¢ Description: SE-ResNeXt50 for medical image classification\n",
      "  ‚Ä¢ Version: 1.0\n",
      "\n",
      "Input/Output Information:\n",
      "  ‚Ä¢ Input 'image':\n",
      "    - Shape: [1, 3, 224, 224]\n",
      "    - Data type: 65568\n",
      "  ‚Ä¢ Output 'output':\n",
      "    - Shape: [1, 1]\n",
      "    - Data type: 65568\n",
      "\n",
      "Testing inference with loaded CoreML model...\n",
      "‚úÖ CoreML inference successful!\n",
      "Inference time: 31.98 ms\n",
      "Output shape: (1, 1)\n",
      "Output value: [[0.89990234]]\n",
      "Sigmoid probability: [[0.71092945]]\n",
      "\n",
      "Comparing with PyTorch model...\n",
      "Output difference: 0.00963944\n",
      "Probability difference: 0.00198501\n",
      "‚ö†Ô∏è  Significant difference detected between CoreML and PyTorch outputs\n",
      "\n",
      "Performance Benchmark (100 inferences):\n",
      "‚úÖ CoreML model loaded successfully!\n",
      "\n",
      "Model Metadata:\n",
      "  ‚Ä¢ Author: PyTorch to CoreML Converter\n",
      "  ‚Ä¢ Description: SE-ResNeXt50 for medical image classification\n",
      "  ‚Ä¢ Version: 1.0\n",
      "\n",
      "Input/Output Information:\n",
      "  ‚Ä¢ Input 'image':\n",
      "    - Shape: [1, 3, 224, 224]\n",
      "    - Data type: 65568\n",
      "  ‚Ä¢ Output 'output':\n",
      "    - Shape: [1, 1]\n",
      "    - Data type: 65568\n",
      "\n",
      "Testing inference with loaded CoreML model...\n",
      "‚úÖ CoreML inference successful!\n",
      "Inference time: 31.98 ms\n",
      "Output shape: (1, 1)\n",
      "Output value: [[0.89990234]]\n",
      "Sigmoid probability: [[0.71092945]]\n",
      "\n",
      "Comparing with PyTorch model...\n",
      "Output difference: 0.00963944\n",
      "Probability difference: 0.00198501\n",
      "‚ö†Ô∏è  Significant difference detected between CoreML and PyTorch outputs\n",
      "\n",
      "Performance Benchmark (100 inferences):\n",
      "  ‚Ä¢ PyTorch average: 30.17 ms\n",
      "  ‚Ä¢ CoreML average: 1.54 ms\n",
      "  ‚Ä¢ CoreML is 19.59x faster!\n",
      "\n",
      "üéâ CoreML conversion and testing completed successfully!\n",
      "  ‚Ä¢ PyTorch average: 30.17 ms\n",
      "  ‚Ä¢ CoreML average: 1.54 ms\n",
      "  ‚Ä¢ CoreML is 19.59x faster!\n",
      "\n",
      "üéâ CoreML conversion and testing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test CoreML model loading and inference\n",
    "print(\"Testing CoreML model loading and inference...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Load the saved CoreML model\n",
    "    print(\"Loading CoreML model from disk...\")\n",
    "    loaded_coreml_model = ct.models.MLModel(output_path)\n",
    "    \n",
    "    print(\"‚úÖ CoreML model loaded successfully!\")\n",
    "    \n",
    "    # Display model metadata\n",
    "    spec = loaded_coreml_model.get_spec()\n",
    "    \n",
    "    print(f\"\\nModel Metadata:\")\n",
    "    print(f\"  ‚Ä¢ Author: {loaded_coreml_model.author}\")\n",
    "    print(f\"  ‚Ä¢ Description: {loaded_coreml_model.short_description}\")\n",
    "    print(f\"  ‚Ä¢ Version: {loaded_coreml_model.version}\")\n",
    "    \n",
    "    # Display input/output information\n",
    "    print(f\"\\nInput/Output Information:\")\n",
    "    for input_feature in spec.description.input:\n",
    "        print(f\"  ‚Ä¢ Input '{input_feature.name}':\")\n",
    "        if input_feature.type.HasField('multiArrayType'):\n",
    "            shape = input_feature.type.multiArrayType.shape\n",
    "            print(f\"    - Shape: {list(shape)}\")\n",
    "            print(f\"    - Data type: {input_feature.type.multiArrayType.dataType}\")\n",
    "    \n",
    "    for output_feature in spec.description.output:\n",
    "        print(f\"  ‚Ä¢ Output '{output_feature.name}':\")\n",
    "        if output_feature.type.HasField('multiArrayType'):\n",
    "            shape = output_feature.type.multiArrayType.shape\n",
    "            print(f\"    - Shape: {list(shape)}\")\n",
    "            print(f\"    - Data type: {output_feature.type.multiArrayType.dataType}\")\n",
    "    \n",
    "    # Test inference with the loaded model\n",
    "    print(f\"\\nTesting inference with loaded CoreML model...\")\n",
    "    \n",
    "    # Prepare test input (numpy array)\n",
    "    test_input = dummy_input.numpy()\n",
    "    input_dict = {\"image\": test_input}\n",
    "    \n",
    "    # Run inference\n",
    "    start_time = time.time()\n",
    "    coreml_output = loaded_coreml_model.predict(input_dict)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ CoreML inference successful!\")\n",
    "    print(f\"Inference time: {inference_time*1000:.2f} ms\")\n",
    "    \n",
    "    # Extract output\n",
    "    coreml_result = coreml_output[\"output\"]\n",
    "    print(f\"Output shape: {coreml_result.shape}\")\n",
    "    print(f\"Output value: {coreml_result}\")\n",
    "    \n",
    "    # Apply sigmoid to get probability\n",
    "    coreml_probability = 1 / (1 + np.exp(-coreml_result))\n",
    "    print(f\"Sigmoid probability: {coreml_probability}\")\n",
    "    \n",
    "    # Compare with original PyTorch model\n",
    "    print(f\"\\nComparing with PyTorch model...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pytorch_output = model(dummy_input)\n",
    "        pytorch_probability = torch.sigmoid(pytorch_output)\n",
    "    \n",
    "    # Calculate differences\n",
    "    output_diff = np.abs(coreml_result - pytorch_output.numpy())\n",
    "    prob_diff = np.abs(coreml_probability - pytorch_probability.numpy())\n",
    "    \n",
    "    print(f\"Output difference: {output_diff.max():.8f}\")\n",
    "    print(f\"Probability difference: {prob_diff.max():.8f}\")\n",
    "    \n",
    "    if output_diff.max() < 1e-4:\n",
    "        print(\"‚úÖ CoreML and PyTorch outputs match closely!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Significant difference detected between CoreML and PyTorch outputs\")\n",
    "    \n",
    "    # Performance benchmark\n",
    "    print(f\"\\nPerformance Benchmark (100 inferences):\")\n",
    "    \n",
    "    # PyTorch benchmark\n",
    "    pytorch_times = []\n",
    "    model.eval()\n",
    "    for _ in range(100):\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "        pytorch_times.append(time.time() - start_time)\n",
    "    \n",
    "    avg_pytorch_time = np.mean(pytorch_times) * 1000  # Convert to ms\n",
    "    \n",
    "    # CoreML benchmark\n",
    "    coreml_times = []\n",
    "    for _ in range(100):\n",
    "        start_time = time.time()\n",
    "        _ = loaded_coreml_model.predict(input_dict)\n",
    "        coreml_times.append(time.time() - start_time)\n",
    "    \n",
    "    avg_coreml_time = np.mean(coreml_times) * 1000  # Convert to ms\n",
    "    \n",
    "    print(f\"  ‚Ä¢ PyTorch average: {avg_pytorch_time:.2f} ms\")\n",
    "    print(f\"  ‚Ä¢ CoreML average: {avg_coreml_time:.2f} ms\")\n",
    "    \n",
    "    if avg_coreml_time < avg_pytorch_time:\n",
    "        speedup = avg_pytorch_time / avg_coreml_time\n",
    "        print(f\"  ‚Ä¢ CoreML is {speedup:.2f}x faster!\")\n",
    "    else:\n",
    "        slowdown = avg_coreml_time / avg_pytorch_time\n",
    "        print(f\"  ‚Ä¢ CoreML is {slowdown:.2f}x slower\")\n",
    "    \n",
    "except Exception as test_error:\n",
    "    print(f\"‚ùå CoreML model testing failed: {test_error}\")\n",
    "    import traceback\n",
    "    print(f\"Traceback: {traceback.format_exc()}\")\n",
    "    raise test_error\n",
    "\n",
    "print(f\"\\nüéâ CoreML conversion and testing completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyworks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
